{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect data from outside sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://import.io/ to get school address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pygeocoder as pgeo\n",
    "import os\n",
    "%matplotlib inline\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data taken from usnews on school which contain addresses - Convert them to Lat Lon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_scrape = pd.read_csv('usnews_schools.csv')\n",
    "df_scrape['address'] = df_scrape.school_value_1.str.lower()+', '+df_scrape.school_value_2.str.lower()\n",
    "df_scrape['geo'] = df_scrape['address'].apply(lambda x: pgeo.Geocoder.geocode(x).coordinates)\n",
    "df_scrape['geolat'] = df_scrape['geo'].apply(lambda x: x[0])\n",
    "df_scrape['geolon'] = df_scrape['geo'].apply(lambda x: x[1]) \n",
    "df_scrape['school_link_1/_text'] = df_scrape['school_link_1/_text'].apply(lambda x: x.decode('utf8').encode('ascii','ignore'))\n",
    "df_scrape_geos = df_scrape[['geolat','geolon','school_link_1/_text']]\n",
    "df_scrape_geos.columns = ['geolat','geolon','School Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lat1 = minlat\n",
    "lat2 = maxlat\n",
    "lon1 = minlon\n",
    "lon2 = maxlon\n",
    "df_scrape_geos = df_scrape_geos[(df_scrape_geos.geolat >= lat1) & (df_scrape_geos.geolat <= lat2) &\n",
    "                                (df_scrape_geos.geolon >= lon1) & (df_scrape_geos.geolon <= lon2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data files from school csvs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_scores(fname):\n",
    "    df_scoress =pd.read_excel(fname)\n",
    "    df_scoress = df_scoress[(df_scoress.County=='County1')|(df_scoress.County=='County2')]\n",
    "    return df_scoress\n",
    "\n",
    "\n",
    "scoressf = [f for f in os.listdir('.') if f[:3]=='xxx']\n",
    "first = True\n",
    "df_scores = None\n",
    "for f in scoressf:\n",
    "    if first:\n",
    "        df_scores = process_scores(f)\n",
    "        first = False\n",
    "    else:\n",
    "        tempdf = process_scores(f)\n",
    "        df_scores = pd.concat([df_scores, tempdf],axis=0)\n",
    "school_df = pd.merge(df_scores, df_scrape_geos, on='School Name', how = 'inner')    \n",
    "school_df.replace('*',np.nan,inplace=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter to interesting stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "school_df = school_df[['geolat','geolon','School Name', 'County', \n",
    "                                   'Charter School','Fiscal Year', 'Fiscal Year',\n",
    "                                   'Math Mean Scale Score','Reading Mean Scale Score',\n",
    "                                   'Writing Mean Scale Score','Science Mean Scale Score']]\n",
    "school_df.dropna(axis=0,inplace=True)\n",
    "school_df = school_df[~school_df['School Name'].apply(lambda x: ('online' in x.lower()) | ('virtual' in x.lower()))]\n",
    "school_df['Total Score'] = school_df[['Math Mean Scale Score','Reading Mean Scale Score','Writing Mean Scale Score','Science Mean Scale Score']].astype(float).sum(axis=1, numeric_only=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dated nn to get associated distances and score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "school_nn10 = KNeighborsRegressor(n_neighbors=1, metric='haversine')\n",
    "school_nn11 = KNeighborsRegressor(n_neighbors=1, metric='haversine')\n",
    "school_nn12 = KNeighborsRegressor(n_neighbors=1, metric='haversine')\n",
    "school_nn13 = KNeighborsRegressor(n_neighbors=1, metric='haversine')\n",
    "school_nn14 = KNeighborsRegressor(n_neighbors=1, metric='haversine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schools = [school_nn10, school_nn11, school_nn12, school_nn13, school_nn14]\n",
    "dates = [2010, 2011, 2012, 2013, 2014]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#School"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for s, d in zip(schools, dates):\n",
    "    dftemp = dfschool[(dfschool['Year']==d) & (dfschool['Charter School']=='N')].copy()\n",
    "    s.fit(dftemp[['geolat','geolon']],dftemp['Total Score'].values)    \n",
    "with open('school_nn', \"wb\") as f:\n",
    "    pickle.dump(schools, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect census block information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_query_grid(lat1 = minlat\n",
    "                        lat2 = maxlat\n",
    "                        lon1 = minlon\n",
    "                        lon2 = maxlon,\n",
    "                        gridspace = 25.0):\n",
    "    gridspacelat = abs(lat1-lat2)/gridspace\n",
    "    gridspacelon = abs(lon2-lon1)/gridspace\n",
    "    grid = []\n",
    "    for la in np.arange(lat1, lat2+gridspacelat, gridspacelat):\n",
    "        for lo in np.arange(lon2-gridspacelon, lon1, gridspacelon):\n",
    "            bnds = (la, lo)\n",
    "            grid.append(bnds)\n",
    "    print 'total generated by grid = {}'.format((gridspace+1)**2)\n",
    "    return grid\n",
    "\n",
    "def latlon_to_fips(lat, lon):\n",
    "    lat = str(np.round(lat,4))\n",
    "    lon = str(np.round(lon,4))    \n",
    "    result = requests.get('http://data.fcc.gov/api/block/find?format=json&latitude='+lat+'&longitude='+lon+'&showall=false').json()\n",
    "    return result['Block']['FIPS']\n",
    "\n",
    "def extract_fip_from_grid(grid):\n",
    "    fiplatlon = []\n",
    "    j=0\n",
    "    for lat, lon in grid:\n",
    "        j+=1\n",
    "        fips = latlon_to_fips(lat, lon)\n",
    "        fiplatlon.append([fips, lat, lon])\n",
    "        print j\n",
    "    return fiplatlon\n",
    "\n",
    "def break_fips(fips):\n",
    "    state = fips[0:2]\n",
    "    county = fips[2:5]\n",
    "    tract = fips[5:11]\n",
    "    block_group = fips[11:12]\n",
    "    block = fips[12:15]\n",
    "    return state, county, tract, block_group, block\n",
    "\n",
    "grid = generate_query_grid()\n",
    "fiplatlon = extract_fip_from_grid(grid)\n",
    "fiplatlon = np.array(fiplatlon, dtype=object)\n",
    "dftemp = pd.DataFrame(fiplatlon)\n",
    "dftemp.columns=['fips','lat','lon']\n",
    "splitfips = np.array(list(dftemp.fips.apply(break_fips).values),dtype=str)\n",
    "dfcensus = pd.concat([pd.DataFrame(splitfips, \n",
    "             columns=['state', 'county', 'tract', 'block_group', 'block']),\n",
    "            dftemp],axis=1)\n",
    "dfcensus = dfcensus[(dfcensus['county']==county1) | (dfcensus['county']==county2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# acs5\n",
    "median_household_income = 'B19013_001E'\n",
    "income_per_capita = 'B19301_001E'\n",
    "median_age = 'B01002_001E'\n",
    "total_transportation = 'B08301_001E'\n",
    "car_transportation = 'B08301_002E'\n",
    "public_transportation = 'B08301_010E'\n",
    "total_living = 'B25003_001E'\n",
    "owner_living = 'B25003_002E'\n",
    "renter_living  = 'B25003_003E'\n",
    "rent_cost = 'B25064_001E'\n",
    "# counties\n",
    "# state\n",
    "# survey\n",
    "acs = 'acs5?'\n",
    "sf = 'sf1?'\n",
    "# combine\n",
    "acslist = [median_household_income,income_per_capita,median_age,\n",
    "           total_transportation,car_transportation,public_transportation,\n",
    "           total_living,owner_living,renter_living,poptotal]\n",
    "col =     ['median_household_income','income_per_capita', 'median_age', \n",
    "           'total_transportation', 'car_transportation', 'public_transportation',\n",
    "           'total_living', 'owner_living', 'renter_living', 'poptotal']\n",
    "acsstr = ','.join(acslist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acs_out={}\n",
    "counties = [county1, county2]\n",
    "year = ['2010/','2011/','2012/','2013/']\n",
    "base = 'http://api.census.gov/data/'\n",
    "key = 'key='\n",
    "get = '&get='\n",
    "location = '&for=block+group:*&in=state:04+county:'\n",
    "\n",
    "for c in counties:\n",
    "    for y in year:\n",
    "        q = base+y+acs+key+get+acsstr+location+c\n",
    "        print q\n",
    "        r = requests.get(q)\n",
    "        acs_out[y[:-1]+'_'+c] = jsonloads(r.text)\n",
    "df_bgrp_census = pd.DataFrame()\n",
    "\n",
    "for key, item in acs_out.iteritems():\n",
    "    dftemp = pd.DataFrame(item,dtype=object, columns=col+['state', 'county', 'tract', 'block_group']).ix[1:]\n",
    "    dftemp['year'] = int(key[0:4])\n",
    "    df_bgrp_census = pd.concat([df_bgrp_census,dftemp],axis=0)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_census_data = pd.merge(dfcensus, df_bgrp_census, on=['state','county','tract','block_group'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_census_data['countytract'] = df_census_data.county + df_census_data.tract\n",
    "df_census_data['countytractblock_group'] = df_census_data.county + df_census_data.tract + df_census_data.block_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nan_fill_with_nn(df, date, metrics = ['geolat','geolon'], years):\n",
    "    if df.isnull().sum().sum()==0:\n",
    "        return df\n",
    "    \n",
    "    df_good = df.dropna(axis=0, inplace=False).reset_index(drop=True).copy()\n",
    "    for date in df.year.unique():\n",
    "        df_bad = df.loc[(df.isnull().sum(axis=1)>0),:].reset_index(drop=True).copy()\n",
    "        df_bad = [df_bad['year'] == date]\n",
    "        nn_good = NearestNeighbors(n_neighbors=1).fit(df_good[df_good['year']=='date'][metrics].apply(np.radians))\n",
    "        key = nn_good.kneighbors(df_bad[metrics].apply(np.radians), return_distance=False)\n",
    "        key = key.flatten()\n",
    "        link = pd.DataFrame(key,columns=['key'])\n",
    "        link = pd.merge(link, df_good, left_on='key',right_index=True, how='left')\n",
    "        link.drop(['key'],axis=1, inplace=True)\n",
    "        df_bad.fillna(link,inplace=True)\n",
    "        df_good = pd.concat([df_good, df_bad], axis=0)    \n",
    "    return df_good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CENSUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_census_data  = nan_fill_with_nn(df_census_data )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_census_data.reset_index(inplace=True, drop=True)\n",
    "nn_census = NearestNeighbors(n_neighbors=1).fit(df_census_data['geolat','geolon'])\n",
    "with open('cenus_nn', \"wb\") as f:\n",
    "    pickle.dump(nn_census, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "yp = pd.read('yelp_with_census_key.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn_census = pd.merge(nn_census, yp, how='right', left_on='key', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schools = [school_nn10, school_nn11, school_nn12, school_nn13, school_nn14]\n",
    "dates = [2010, 2011, 2012, 2013, 2014]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dateschool = dict(zip(dates, schools))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add everything together to a posgress database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engine = create_engine('''''')\n",
    "cs=700\n",
    "data_parcels = pd.read_csv(\n",
    "    dirc_in+'mls_listings.csv',\n",
    "    chunksize = cs,\n",
    "    parse_dates = ['ListDate'],\n",
    "    low_memory = False)\n",
    "\n",
    "data_parcels['key'] = nn_census.kneighbors(data_parcels[['geolat','geolon']], return_distance=False)\n",
    "data_parcels = pd.merge(data_parcels, yp, how='right', left_on='key', right_index=True)\n",
    "data_parcels = pd.merge(df_census_data, yp, how='right', left_on='key', right_index=True)\n",
    "data_parcels['distance_from_school'] = kneighbors(data_parcels[['geolat','geolon']])[1]\n",
    "data_parcels['school_score'] = 0\n",
    "for d in dates:\n",
    "    data_parcels.loc[data_parcels['ListDate'].dt.year == d,'school_distance'] = dateschool[d].kneighbors(data_parcels.loc[data_parcels['ListDate'].dt.year == d,'school_score'])[1][0]\n",
    "    data_parcels.loc[data_parcels['ListDate'].dt.year == d,'school_score'] = dateschool[d].predict(data_parcels.loc[data_parcels['ListDate'].dt.year == d,'school_score'])[1][0]\n",
    "\n",
    "for i, chunk in enumerate(data_parcels):\n",
    "    chunk.columns = map(str.lower, chunk.columns)\n",
    "    chunk.to_sql('listings', engine, if_exists='replace', index=False, chunksize=700)\n",
    "    print '{} amount of rows have been added to databases'.format(str(cs*(i+1)))\n",
    "    break "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
